{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7939f337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca396be9",
   "metadata": {},
   "source": [
    "## 1. XOR 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7795842",
   "metadata": {},
   "source": [
    "#### (1) OR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa0979ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 1, 0], \n",
    "                  [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [1]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0687464",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[8, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), tf.float32, name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "\n",
    "# 가설\n",
    "hypot = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# 비용\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "\n",
    "# 최소 비용\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "143ceec3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.356341  ]\n",
      " [0.92209053]\n",
      " [0.9236759 ]\n",
      " [0.9961498 ]\n",
      " [0.8832964 ]\n",
      " [0.9939922 ]\n",
      " [0.99385774]\n",
      " [0.99971735]]\n",
      "예측 :  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "정확도 :  1.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "preds = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(preds, y), dtype=tf.float32))\n",
    "\n",
    "for step in range(1000):\n",
    "    _, h, p, a = sess.run([train, hypot, preds, accuracy], feed_dict={X:X_data, y:y_data})\n",
    "\n",
    "print(\"가설 : \", h)\n",
    "print(\"예측 : \", p)\n",
    "print(\"정확도 : \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3af80a",
   "metadata": {},
   "source": [
    "#### (2) AND gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd20e954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.01015344]\n",
      " [0.04852104]\n",
      " [0.04406434]\n",
      " [0.18643898]\n",
      " [0.04382184]\n",
      " [0.17077985]\n",
      " [0.18556502]\n",
      " [0.50590295]]\n",
      "예측 :  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "정확도 :  1.0\n"
     ]
    }
   ],
   "source": [
    "X_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 1, 0], \n",
    "                  [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [0], [0], [0], [0], [0], [0], [1]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), tf.float32, name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "\n",
    "# 가설\n",
    "hypot = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# 비용\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "\n",
    "# 최소 비용\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "preds = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(preds, y), dtype=tf.float32))\n",
    "\n",
    "for step in range(1000):\n",
    "    _, h, p, a = sess.run([train, hypot, preds, accuracy], feed_dict={X:X_data, y:y_data})\n",
    "\n",
    "print(\"가설 : \", h)\n",
    "print(\"예측 : \", p)\n",
    "print(\"정확도 : \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c1a2b9",
   "metadata": {},
   "source": [
    "#### (3) XOR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "894ef431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.7500023]\n",
      " [0.7500012]\n",
      " [0.7500012]\n",
      " [0.75     ]\n",
      " [0.7500012]\n",
      " [0.75     ]\n",
      " [0.75     ]\n",
      " [0.7499988]]\n",
      "예측 :  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "정확도 :  0.75\n"
     ]
    }
   ],
   "source": [
    "X_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 1, 0], \n",
    "                  [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), tf.float32, name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias\")\n",
    "\n",
    "# 가설\n",
    "hypot = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# 비용\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "\n",
    "# 최소 비용\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "preds = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(preds, y), dtype=tf.float32))\n",
    "\n",
    "for step in range(10000):\n",
    "    _, h, p, a = sess.run([train, hypot, preds, accuracy], feed_dict={X:X_data, y:y_data})\n",
    "\n",
    "print(\"가설 : \", h)\n",
    "print(\"예측 : \", p)\n",
    "print(\"정확도 : \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a98cc28",
   "metadata": {},
   "source": [
    "#### (4) SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba1c4dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0ea1123",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 1, 0], \n",
    "                  [1, 0, 1], [1, 1, 1]]\n",
    "\n",
    "y = [0, 1, 1, 1, 1, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b094ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(C=100).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36d3d814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "examples = [[0, 0, 0], [1, 1, 1], [0, 1, 0], [1, 0, 0], [1, 1, 0]]\n",
    "exam_label = [0, 0, 1, 1, 1]\n",
    "\n",
    "result = clf.predict(examples)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41b824ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "score = metrics.accuracy_score(exam_label, result)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9aec40",
   "metadata": {},
   "source": [
    "#### (5) 딥러닝을 이용한 XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4d26885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.00991341]\n",
      " [0.99542695]\n",
      " [0.99704576]\n",
      " [0.9924141 ]\n",
      " [0.9978443 ]\n",
      " [0.9902837 ]\n",
      " [0.990001  ]\n",
      " [0.02831692]]\n",
      "예측 :  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "정확도 :  1.0\n"
     ]
    }
   ],
   "source": [
    "X_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 1, 0], \n",
    "                  [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "# 첫번째 레이어\n",
    "W1 = tf.Variable(tf.random_normal([3, 10]), tf.float32, name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([10]), tf.float32, name=\"bias1\")\n",
    "hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# 두번째 레이어\n",
    "W2 = tf.Variable(tf.random_normal([10, 1]), tf.float32, name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias2\")\n",
    "hypot2 = tf.sigmoid(tf.matmul(hypot1, W2) + b2)\n",
    "\n",
    "# 비용\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot2) + (1 - y) * tf.log(1 - hypot2))\n",
    "\n",
    "# 최소 비용\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "preds = tf.cast(hypot2 > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(preds, y), dtype=tf.float32))\n",
    "\n",
    "for step in range(10000):\n",
    "    _, h, p, a = sess.run([train, hypot2, preds, accuracy], feed_dict={X:X_data, y:y_data})\n",
    "\n",
    "print(\"가설 : \", h)\n",
    "print(\"예측 : \", p)\n",
    "print(\"정확도 : \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f00952",
   "metadata": {},
   "source": [
    "#### (6) Deep & Wide\n",
    "\n",
    "+ Deep : 6개의 hidden layer 추가\n",
    "+ Wide : 각 계층의 입출력 갯수는 50개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5beec973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[6.9248676e-04]\n",
      " [9.9966913e-01]\n",
      " [9.9960399e-01]\n",
      " [9.9985504e-01]\n",
      " [9.9990618e-01]\n",
      " [9.9983203e-01]\n",
      " [9.9982345e-01]\n",
      " [5.3167343e-04]]\n",
      "예측 :  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "정확도 :  1.0\n"
     ]
    }
   ],
   "source": [
    "X_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 1, 0], \n",
    "                  [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "# 첫번째 레이어\n",
    "W1 = tf.Variable(tf.random_normal([3, 50]), tf.float32, name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias1\")\n",
    "hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# 두번째 레이어\n",
    "W2 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias2\")\n",
    "hypot2 = tf.sigmoid(tf.matmul(hypot1, W2) + b2)\n",
    "\n",
    "# 세번째 레이어\n",
    "W3 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias3\")\n",
    "hypot3 = tf.sigmoid(tf.matmul(hypot2, W3) + b3)\n",
    "\n",
    "# 네번째 레이어\n",
    "W4 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight4\")\n",
    "b4 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias4\")\n",
    "hypot4 = tf.sigmoid(tf.matmul(hypot3, W4) + b4)\n",
    "\n",
    "# 다섯번째 레이어\n",
    "W5 = tf.Variable(tf.random_normal([50, 50]), tf.float32, name=\"weight5\")\n",
    "b5 = tf.Variable(tf.random_normal([50]), tf.float32, name=\"bias5\")\n",
    "hypot5 = tf.sigmoid(tf.matmul(hypot4, W5) + b5)\n",
    "\n",
    "# 여섯번째 레이어\n",
    "W6 = tf.Variable(tf.random_normal([50, 1]), tf.float32, name=\"weight6\")\n",
    "b6 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias6\")\n",
    "hypot = tf.sigmoid(tf.matmul(hypot5, W6) + b6)\n",
    "\n",
    "# 비용\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "\n",
    "# 최소 비용\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "preds = tf.cast(hypot > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(preds, y), dtype=tf.float32))\n",
    "\n",
    "for step in range(10000):\n",
    "    _, h, p, a = sess.run([train, hypot, preds, accuracy], feed_dict={X:X_data, y:y_data})\n",
    "\n",
    "print(\"가설 : \", h)\n",
    "print(\"예측 : \", p)\n",
    "print(\"정확도 : \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3409af",
   "metadata": {},
   "source": [
    "## 2. Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1452fcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.01884696]\n",
      " [0.99139476]\n",
      " [0.9973155 ]\n",
      " [0.99036217]\n",
      " [0.9925849 ]\n",
      " [0.98790574]\n",
      " [0.99172634]\n",
      " [0.03093642]]\n",
      "예측 :  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "정확도 :  1.0\n"
     ]
    }
   ],
   "source": [
    "X_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 1, 0], \n",
    "                  [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "# 첫번째 레이어\n",
    "W1 = tf.Variable(tf.random_normal([3, 10]), tf.float32, name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([10]), tf.float32, name=\"bias1\")\n",
    "hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# 두번째 레이어\n",
    "W2 = tf.Variable(tf.random_normal([10, 1]), tf.float32, name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias2\")\n",
    "hypot2 = tf.sigmoid(tf.matmul(hypot1, W2) + b2)\n",
    "\n",
    "tf.summary.histogram(\"weight2\", W2)\n",
    "\n",
    "# 비용\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot2) + (1 - y) * tf.log(1 - hypot2))\n",
    "\n",
    "tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "# 최소 비용\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "preds = tf.cast(hypot2 > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(preds, y), dtype=tf.float32))\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"log_dir2/alpha01\")\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "for step in range(10000):\n",
    "    _, h, p, a, m = sess.run([train, hypot2, preds, accuracy, merged_summary], feed_dict={X:X_data, y:y_data})\n",
    "    writer.add_summary(m, global_step=step)\n",
    "    \n",
    "print(\"가설 : \", h)\n",
    "print(\"예측 : \", p)\n",
    "print(\"정확도 : \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b060309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate tf1\n",
    "# tensorboard --logdir=./log_dir2/alpha01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8dc896d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.0130586 ]\n",
      " [0.99186444]\n",
      " [0.99123186]\n",
      " [0.99207604]\n",
      " [0.9871789 ]\n",
      " [0.99247825]\n",
      " [0.9948498 ]\n",
      " [0.03657955]]\n",
      "예측 :  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "정확도 :  1.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 1, 0], \n",
    "                  [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "# 첫번째 레이어\n",
    "with tf.name_scope(\"layer1\"):\n",
    "    W1 = tf.Variable(tf.random_normal([3, 10]), tf.float32, name=\"weight1\")\n",
    "    b1 = tf.Variable(tf.random_normal([10]), tf.float32, name=\"bias1\")\n",
    "    hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "    tf.summary.histogram(\"weight1\", W1)\n",
    "    tf.summary.histogram(\"bias1\", b1)\n",
    "    tf.summary.histogram(\"hypot1\", hypot1)\n",
    "\n",
    "# 두번째 레이어\n",
    "with tf.name_scope(\"layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([10, 1]), tf.float32, name=\"weight2\")\n",
    "    b2 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias2\")\n",
    "    hypot2 = tf.sigmoid(tf.matmul(hypot1, W2) + b2)\n",
    "\n",
    "    tf.summary.histogram(\"weight2\", W2)\n",
    "    tf.summary.histogram(\"bias2\", b2)\n",
    "    tf.summary.histogram(\"hypot2\", hypot2)\n",
    "\n",
    "# 비용\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = -tf.reduce_mean(y * tf.log(hypot2) + (1 - y) * tf.log(1 - hypot2))\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "# 최소 비용\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    preds = tf.cast(hypot2 > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(preds, y), dtype=tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"log_dir2/alpha01\")\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "for step in range(10000):\n",
    "    _, h, p, a, m = sess.run([train, hypot2, preds, accuracy, merged_summary], feed_dict={X:X_data, y:y_data})\n",
    "    writer.add_summary(m, global_step=step)\n",
    "    \n",
    "print(\"가설 : \", h)\n",
    "print(\"예측 : \", p)\n",
    "print(\"정확도 : \", a)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "092a3dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설 :  [[0.3775971 ]\n",
      " [0.8221923 ]\n",
      " [0.842606  ]\n",
      " [0.8229219 ]\n",
      " [0.8317431 ]\n",
      " [0.83234143]\n",
      " [0.8286257 ]\n",
      " [0.64307797]]\n",
      "예측 :  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "정확도 :  0.875\n"
     ]
    }
   ],
   "source": [
    "# learning_rate = 0.01\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 1, 0], \n",
    "                  [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[8, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[8, 1])\n",
    "\n",
    "# 첫번째 레이어\n",
    "with tf.name_scope(\"layer1\"):\n",
    "    W1 = tf.Variable(tf.random_normal([3, 10]), tf.float32, name=\"weight1\")\n",
    "    b1 = tf.Variable(tf.random_normal([10]), tf.float32, name=\"bias1\")\n",
    "    hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "    tf.summary.histogram(\"weight1\", W1)\n",
    "    tf.summary.histogram(\"bias1\", b1)\n",
    "    tf.summary.histogram(\"hypot1\", hypot1)\n",
    "\n",
    "# 두번째 레이어\n",
    "with tf.name_scope(\"layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([10, 1]), tf.float32, name=\"weight2\")\n",
    "    b2 = tf.Variable(tf.random_normal([1]), tf.float32, name=\"bias2\")\n",
    "    hypot2 = tf.sigmoid(tf.matmul(hypot1, W2) + b2)\n",
    "\n",
    "    tf.summary.histogram(\"weight2\", W2)\n",
    "    tf.summary.histogram(\"bias2\", b2)\n",
    "    tf.summary.histogram(\"hypot2\", hypot2)\n",
    "\n",
    "# 비용\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = -tf.reduce_mean(y * tf.log(hypot2) + (1 - y) * tf.log(1 - hypot2))\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "# 최소 비용\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    preds = tf.cast(hypot2 > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(preds, y), dtype=tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"log_dir2/alpha001\")\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "for step in range(10000):\n",
    "    _, h, p, a, m = sess.run([train, hypot2, preds, accuracy, merged_summary], feed_dict={X:X_data, y:y_data})\n",
    "    writer.add_summary(m, global_step=step)\n",
    "    \n",
    "print(\"가설 : \", h)\n",
    "print(\"예측 : \", p)\n",
    "print(\"정확도 : \", a)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3115723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate tf1\n",
    "# tensorboard --logdir=./log_dir2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6001cf",
   "metadata": {},
   "source": [
    "## 3.ReLU : Rectified Linear Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff2d830",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-84fb6f4620c0>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\acorn\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(777)\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b76e271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x00000159F6D0C1C8>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x00000159F6D977C8>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x00000159F6D31AC8>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f02450",
   "metadata": {},
   "source": [
    "#### (1) 첫번째 모델 구축 : 성능 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a187021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# 가설\n",
    "logit = tf.matmul(X, W) + b\n",
    "hypot = tf.nn.softmax(logit)\n",
    "\n",
    "# 비용\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y))\n",
    "\n",
    "# 최소 비용\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb879587",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = tf.equal(tf.argmax(hypot, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d71a94f5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  1     cost :  4.10477936701341\n",
      "epoch :  2     cost :  1.531863405487754\n",
      "epoch :  3     cost :  1.14346436955712\n",
      "epoch :  4     cost :  0.9695856538685892\n",
      "epoch :  5     cost :  0.8675219155441629\n",
      "epoch :  6     cost :  0.7994823081926866\n",
      "epoch :  7     cost :  0.7495254193652756\n",
      "epoch :  8     cost :  0.7107486788793047\n",
      "epoch :  9     cost :  0.6795224343646651\n",
      "epoch :  10     cost :  0.6534588269212027\n",
      "epoch :  11     cost :  0.631521351879293\n",
      "epoch :  12     cost :  0.612143087387085\n",
      "epoch :  13     cost :  0.5951191742853683\n",
      "epoch :  14     cost :  0.5800072607127104\n",
      "epoch :  15     cost :  0.5665797857262871\n",
      "epoch :  16     cost :  0.5544712929292163\n",
      "epoch :  17     cost :  0.5437802591107108\n",
      "epoch :  18     cost :  0.5333310548825699\n",
      "epoch :  19     cost :  0.5239941353147681\n",
      "epoch :  20     cost :  0.515262087583542\n",
      "epoch :  21     cost :  0.5073459146239543\n",
      "epoch :  22     cost :  0.49971502493728265\n",
      "epoch :  23     cost :  0.4929727880521256\n",
      "epoch :  24     cost :  0.4863550806045531\n",
      "epoch :  25     cost :  0.48001020794565047\n",
      "epoch :  26     cost :  0.47423703226176184\n",
      "epoch :  27     cost :  0.46910761665214196\n",
      "epoch :  28     cost :  0.4636474550854078\n",
      "epoch :  29     cost :  0.4585560624707829\n",
      "epoch :  30     cost :  0.45386759124018927\n",
      "epoch :  31     cost :  0.4494578668746084\n",
      "epoch :  32     cost :  0.4451386751369997\n",
      "epoch :  33     cost :  0.4408469946276056\n",
      "epoch :  34     cost :  0.4369340027462353\n",
      "epoch :  35     cost :  0.4331618681279097\n",
      "epoch :  36     cost :  0.42965442689982364\n",
      "epoch :  37     cost :  0.42602899627252044\n",
      "epoch :  38     cost :  0.42280255339362405\n",
      "epoch :  39     cost :  0.41958797693252564\n",
      "epoch :  40     cost :  0.41636307683857987\n",
      "epoch :  41     cost :  0.41361823363737615\n",
      "epoch :  42     cost :  0.410500750595873\n",
      "epoch :  43     cost :  0.4076165154847231\n",
      "epoch :  44     cost :  0.40498332224108957\n",
      "epoch :  45     cost :  0.40207061938264166\n",
      "epoch :  46     cost :  0.3997957016663117\n",
      "epoch :  47     cost :  0.3972002811323514\n",
      "epoch :  48     cost :  0.3948619361357254\n",
      "epoch :  49     cost :  0.392528745532036\n",
      "epoch :  50     cost :  0.3903146905790674\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 50\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c / total_batch\n",
    "        \n",
    "    print(\"epoch : \", (epoch+1), \"    cost : \", avg_cost)\n",
    "\n",
    "print(\"훈련 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "813df2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.9046\n"
     ]
    }
   ],
   "source": [
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a42e335",
   "metadata": {},
   "source": [
    "#### (2) 딥러닝으로 모델 구현 : 성능 87% ~ 88%\n",
    "\n",
    "+ Deep : 레이어는 7개 추가\n",
    "+ Wide : 계층간 입출력 갯수는 256개 \n",
    "+ activation function : relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89c2a472",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  1     cost :  2.4339594021710496\n",
      "epoch :  2     cost :  1.518996687368912\n",
      "epoch :  3     cost :  1.3056070280075078\n",
      "epoch :  4     cost :  1.1500847389481283\n",
      "epoch :  5     cost :  1.0468383613499723\n",
      "epoch :  6     cost :  1.0207318869504063\n",
      "epoch :  7     cost :  0.9821508888764813\n",
      "epoch :  8     cost :  0.9505476910417723\n",
      "epoch :  9     cost :  0.8981672555750069\n",
      "epoch :  10     cost :  0.8298684679378169\n",
      "epoch :  11     cost :  0.8065687565370047\n",
      "epoch :  12     cost :  0.7683234505219894\n",
      "epoch :  13     cost :  0.7334391390193589\n",
      "epoch :  14     cost :  0.7507161355018613\n",
      "epoch :  15     cost :  0.7378097633882005\n",
      "epoch :  16     cost :  0.6762615144252779\n",
      "epoch :  17     cost :  0.6607129421017385\n",
      "epoch :  18     cost :  0.6539074406840586\n",
      "epoch :  19     cost :  0.6430997860431675\n",
      "epoch :  20     cost :  0.640245162898844\n",
      "epoch :  21     cost :  0.6215953106229954\n",
      "epoch :  22     cost :  0.6225667694481931\n",
      "epoch :  23     cost :  0.6122563456405294\n",
      "epoch :  24     cost :  0.586254062869332\n",
      "epoch :  25     cost :  0.561472748843106\n",
      "epoch :  26     cost :  0.5402314750714737\n",
      "epoch :  27     cost :  0.5288538791916586\n",
      "epoch :  28     cost :  0.5047109515016731\n",
      "epoch :  29     cost :  0.5018196241422134\n",
      "epoch :  30     cost :  0.490143395120447\n",
      "epoch :  31     cost :  0.4764987274733456\n",
      "epoch :  32     cost :  0.4658412326465954\n",
      "epoch :  33     cost :  0.4638488032601096\n",
      "epoch :  34     cost :  0.4667352743582292\n",
      "epoch :  35     cost :  0.46941435824741035\n",
      "epoch :  36     cost :  0.445316798470237\n",
      "epoch :  37     cost :  0.43688605709509426\n",
      "epoch :  38     cost :  0.4361775742877615\n",
      "epoch :  39     cost :  0.4363831850615415\n",
      "epoch :  40     cost :  0.4379649688980796\n",
      "epoch :  41     cost :  0.4402948172525924\n",
      "epoch :  42     cost :  0.42835671804168\n",
      "epoch :  43     cost :  0.41248894810676595\n",
      "epoch :  44     cost :  0.406117965633219\n",
      "epoch :  45     cost :  0.4048865800554101\n",
      "epoch :  46     cost :  0.407856648781083\n",
      "epoch :  47     cost :  0.41111263237216267\n",
      "epoch :  48     cost :  0.4026952017437328\n",
      "epoch :  49     cost :  0.39057432413101184\n",
      "epoch :  50     cost :  0.37914730922742285\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "# Layer1\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit1 = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit1)\n",
    "\n",
    "# Layer2\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit2 = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit2)\n",
    "\n",
    "# Layer3\n",
    "W3 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit3 = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.nn.sigmoid(logit3)\n",
    "\n",
    "# Layer4\n",
    "W4 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b4 = tf.Variable(tf.random_normal([256]))\n",
    "logit4 = tf.matmul(layer3, W4) + b4\n",
    "layer4 = tf.nn.sigmoid(logit4)\n",
    "\n",
    "# Layer5\n",
    "W5 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b5 = tf.Variable(tf.random_normal([256]))\n",
    "logit5 = tf.matmul(layer4, W5) + b5\n",
    "layer5 = tf.nn.sigmoid(logit5)\n",
    "\n",
    "# Layer6\n",
    "W6 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b6 = tf.Variable(tf.random_normal([256]))\n",
    "logit6 = tf.matmul(layer5, W6) + b6\n",
    "layer6 = tf.nn.sigmoid(logit6)\n",
    "\n",
    "# Layer7\n",
    "W7 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b7 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer6, W7) + b7\n",
    "hypot = tf.nn.softmax(logit)\n",
    "\n",
    "# 비용\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y))\n",
    "\n",
    "# 최소 비용\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "correct = tf.equal(tf.argmax(hypot, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 50\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c / total_batch\n",
    "        \n",
    "    print(\"epoch : \", (epoch+1), \"    cost : \", avg_cost)\n",
    "\n",
    "print(\"훈련 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99ee9f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.8856\n"
     ]
    }
   ],
   "source": [
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1435b3f0",
   "metadata": {},
   "source": [
    "#### (3) Xavier 초기화 : 96% ~ 97%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19cb6835",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "epoch :  1     cost :  2.3198122856833723\n",
      "epoch :  2     cost :  2.3143902518532493\n",
      "epoch :  3     cost :  2.312439812746917\n",
      "epoch :  4     cost :  2.3122422469745976\n",
      "epoch :  5     cost :  2.310888904224744\n",
      "epoch :  6     cost :  2.310202883807096\n",
      "epoch :  7     cost :  2.3097774644331492\n",
      "epoch :  8     cost :  2.308515960519965\n",
      "epoch :  9     cost :  2.307718233628707\n",
      "epoch :  10     cost :  2.3065167661146675\n",
      "epoch :  11     cost :  2.3053476559032102\n",
      "epoch :  12     cost :  2.3033011167699646\n",
      "epoch :  13     cost :  2.2980156941847385\n",
      "epoch :  14     cost :  2.2766503750194205\n",
      "epoch :  15     cost :  2.028555090210654\n",
      "epoch :  16     cost :  1.7095228927785697\n",
      "epoch :  17     cost :  1.5673081254959103\n",
      "epoch :  18     cost :  1.430278531421315\n",
      "epoch :  19     cost :  1.2919160066951405\n",
      "epoch :  20     cost :  1.1743000481345438\n",
      "epoch :  21     cost :  1.0264950312267644\n",
      "epoch :  22     cost :  0.8335685662789778\n",
      "epoch :  23     cost :  0.6319259990345346\n",
      "epoch :  24     cost :  0.4848557596856899\n",
      "epoch :  25     cost :  0.4050287986343554\n",
      "epoch :  26     cost :  0.3324907807870345\n",
      "epoch :  27     cost :  0.2810467138615521\n",
      "epoch :  28     cost :  0.24526024734432061\n",
      "epoch :  29     cost :  0.21601980632001697\n",
      "epoch :  30     cost :  0.19266084405508913\n",
      "epoch :  31     cost :  0.17492493493990463\n",
      "epoch :  32     cost :  0.15742095059969208\n",
      "epoch :  33     cost :  0.1422856381941926\n",
      "epoch :  34     cost :  0.12748986417596997\n",
      "epoch :  35     cost :  0.11673752806403426\n",
      "epoch :  36     cost :  0.10910391890867191\n",
      "epoch :  37     cost :  0.09957812219180845\n",
      "epoch :  38     cost :  0.09007303636182439\n",
      "epoch :  39     cost :  0.08320329798893493\n",
      "epoch :  40     cost :  0.07737282398749484\n",
      "epoch :  41     cost :  0.07059384745630352\n",
      "epoch :  42     cost :  0.06242417881434611\n",
      "epoch :  43     cost :  0.058552713705734796\n",
      "epoch :  44     cost :  0.05345417984507298\n",
      "epoch :  45     cost :  0.04984231227162213\n",
      "epoch :  46     cost :  0.0467780030349439\n",
      "epoch :  47     cost :  0.04107878512279554\n",
      "epoch :  48     cost :  0.037248606776649304\n",
      "epoch :  49     cost :  0.03499095193364403\n",
      "epoch :  50     cost :  0.03198957146738063\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "# Layer1\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit1 = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit1)\n",
    "\n",
    "# Layer2\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit2 = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit2)\n",
    "\n",
    "# Layer3\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit3 = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.nn.sigmoid(logit3)\n",
    "\n",
    "# Layer4\n",
    "W4 = tf.get_variable(\"W4\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([256]))\n",
    "logit4 = tf.matmul(layer3, W4) + b4\n",
    "layer4 = tf.nn.sigmoid(logit4)\n",
    "\n",
    "# Layer5\n",
    "W5 = tf.get_variable(\"W5\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([256]))\n",
    "logit5 = tf.matmul(layer4, W5) + b5\n",
    "layer5 = tf.nn.sigmoid(logit5)\n",
    "\n",
    "# Layer6\n",
    "W6 = tf.get_variable(\"W6\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([256]))\n",
    "logit6 = tf.matmul(layer5, W6) + b6\n",
    "layer6 = tf.nn.sigmoid(logit6)\n",
    "\n",
    "# Layer7\n",
    "W7 = tf.get_variable(\"W7\", shape=[256, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer6, W7) + b7\n",
    "hypot = tf.nn.softmax(logit)\n",
    "\n",
    "# 비용\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y))\n",
    "\n",
    "# 최소 비용\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "correct = tf.equal(tf.argmax(hypot, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 50\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c / total_batch\n",
    "        \n",
    "    print(\"epoch : \", (epoch+1), \"    cost : \", avg_cost)\n",
    "\n",
    "print(\"훈련 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1221b8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  0.9662\n"
     ]
    }
   ],
   "source": [
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8c5b10",
   "metadata": {},
   "source": [
    "#### (4) Dropout + Adam : 98.2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "473f1116",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  1     cost :  0.8216594354672869\n",
      "epoch :  2     cost :  0.19889767741615136\n",
      "epoch :  3     cost :  0.13627950870177963\n",
      "epoch :  4     cost :  0.10297476425766941\n",
      "epoch :  5     cost :  0.08243232685225928\n",
      "epoch :  6     cost :  0.06983436575328765\n",
      "epoch :  7     cost :  0.05966539546678012\n",
      "epoch :  8     cost :  0.04905549890361726\n",
      "epoch :  9     cost :  0.041507706472819504\n",
      "epoch :  10     cost :  0.04071617623240772\n",
      "epoch :  11     cost :  0.03653871257450771\n",
      "epoch :  12     cost :  0.02982809353111821\n",
      "epoch :  13     cost :  0.02880789266645232\n",
      "epoch :  14     cost :  0.026646013387732887\n",
      "epoch :  15     cost :  0.02560255338903518\n",
      "epoch :  16     cost :  0.022228200253924704\n",
      "epoch :  17     cost :  0.024533430882941246\n",
      "epoch :  18     cost :  0.02162797694004521\n",
      "epoch :  19     cost :  0.021157417354459696\n",
      "epoch :  20     cost :  0.019893249630377714\n",
      "epoch :  21     cost :  0.015830750152923755\n",
      "epoch :  22     cost :  0.018441555345251055\n",
      "epoch :  23     cost :  0.015809565605754448\n",
      "epoch :  24     cost :  0.016878506837952466\n",
      "epoch :  25     cost :  0.01380997408557132\n",
      "epoch :  26     cost :  0.015053267068684683\n",
      "epoch :  27     cost :  0.011789824797385582\n",
      "epoch :  28     cost :  0.015587597716234582\n",
      "epoch :  29     cost :  0.014143376379771241\n",
      "epoch :  30     cost :  0.014266532077665693\n",
      "epoch :  31     cost :  0.01852348403404572\n",
      "epoch :  32     cost :  0.010412063498423035\n",
      "epoch :  33     cost :  0.011782420626497538\n",
      "epoch :  34     cost :  0.014349700209687316\n",
      "epoch :  35     cost :  0.013829129920856999\n",
      "epoch :  36     cost :  0.011967327341421464\n",
      "epoch :  37     cost :  0.00884382028746503\n",
      "epoch :  38     cost :  0.013380010967078054\n",
      "epoch :  39     cost :  0.009463770121728884\n",
      "epoch :  40     cost :  0.012344634970623618\n",
      "epoch :  41     cost :  0.015614271452802551\n",
      "epoch :  42     cost :  0.010867781678385157\n",
      "epoch :  43     cost :  0.010347052254803868\n",
      "epoch :  44     cost :  0.013040632747122968\n",
      "epoch :  45     cost :  0.00860402064655897\n",
      "epoch :  46     cost :  0.012196493516239583\n",
      "epoch :  47     cost :  0.012641324902096218\n",
      "epoch :  48     cost :  0.010351907032947399\n",
      "epoch :  49     cost :  0.010570344123579241\n",
      "epoch :  50     cost :  0.010876741349187457\n",
      "훈련 종료\n",
      "정확도 :  0.9821\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Layer1\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit1 = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.relu(logit1)\n",
    "#layer1 = tf.nn.dropout(layer1, keep_prob=prob)\n",
    "\n",
    "# Layer2\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit2 = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.relu(logit2)\n",
    "layer2 = tf.nn.dropout(layer2, keep_prob=prob)\n",
    "\n",
    "# Layer3\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit3 = tf.matmul(layer2, W3) + b3\n",
    "layer3 = tf.nn.sigmoid(logit3)\n",
    "layer3 = tf.nn.dropout(layer3, keep_prob=prob)\n",
    "\n",
    "# Layer4\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit4 = tf.matmul(layer3, W4) + b4\n",
    "layer4 = tf.nn.relu(logit4)\n",
    "layer4 = tf.nn.dropout(layer4, keep_prob=prob)\n",
    "\n",
    "# Layer5\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit5 = tf.matmul(layer4, W5) + b5\n",
    "layer5 = tf.nn.relu(logit5)\n",
    "layer5 = tf.nn.dropout(layer5, keep_prob=prob)\n",
    "\n",
    "# Layer6\n",
    "W6 = tf.get_variable(\"W6\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit6 = tf.matmul(layer5, W6) + b6\n",
    "layer6 = tf.nn.relu(logit6)\n",
    "layer6 = tf.nn.dropout(layer6, keep_prob=prob)\n",
    "\n",
    "# Layer7\n",
    "W7 = tf.get_variable(\"W7\", shape=[512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([10]))\n",
    "logit = tf.matmul(layer6, W7) + b7\n",
    "hypot = tf.nn.softmax(logit)\n",
    "hypot = tf.nn.dropout(hypot, keep_prob=prob)\n",
    "\n",
    "# 비용\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y))\n",
    "\n",
    "# 최소 비용\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "correct = tf.equal(tf.argmax(hypot, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 50\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys, prob:0.7})\n",
    "        avg_cost += c / total_batch\n",
    "        \n",
    "    print(\"epoch : \", (epoch+1), \"    cost : \", avg_cost)\n",
    "\n",
    "print(\"훈련 종료\")\n",
    "\n",
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images, y:mnist.test.labels, prob:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce09854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
